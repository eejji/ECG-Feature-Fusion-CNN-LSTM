{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1573c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Conv1D,TimeDistributed, Reshape, Activation, add, Layer, Concatenate, GRU, Bidirectional \n",
    "from tensorflow.keras.layers import MaxPooling1D, BatchNormalization, Dropout,Activation, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9781abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-peak detection\n",
    "def get_rpeak(signal, sampling_rate):\n",
    "    _, rpeaks = nk.ecg_peaks(signal, sampling_rate=sampling_rate, show=False, method='neurokit')\n",
    "    \n",
    "    return rpeaks['ECG_R_Peaks']\n",
    "\n",
    "# Segmentation Function\n",
    "\n",
    "def load_BEAT_data(signal, r_peaks):\n",
    "    signal_list = []\n",
    "    for number in range(1,len(r_peaks)-1):\n",
    "        signal_list.append(signal[r_peaks[number]-(int(left_sec*sampling_rate)) : r_peaks[number]+(int(right_sec*sampling_rate))])\n",
    "    return np.array(signal_list)\n",
    "\n",
    "def load_RHYTHM_data(data, rpeak, window):\n",
    "    rhythm_data = []\n",
    "    right = int(right_sec * sampling_rate)\n",
    "    for i in range(1,len(rpeak)-1):\n",
    "        if len(data[rpeak[i] + right : rpeak[i] + window + right]) == window:\n",
    "            rhythm_data.append(data[rpeak[i] + right : rpeak[i]+right + window])\n",
    "\n",
    "    return np.array(rhythm_data)\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return data[indices], labels[indices]\n",
    "\n",
    "def shuffle_data2(data1, data2, labels):\n",
    "    # data1, data2, label을 입력받아 무작위로 섞고 (인덱스 위치는 동일하게) Return\n",
    "    indices = np.arange(data1.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return data1[indices], data2[indices], labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b125dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_class_weight(label):\n",
    "    \n",
    "    counts = np.bincount(label)\n",
    "    \n",
    "    amuse_num = counts[0]\n",
    "    normal_num = counts[1]\n",
    "    stress_num = counts[2]\n",
    "    meditation_num = counts[3]\n",
    "    total = amuse_num + normal_num + stress_num + meditation_num\n",
    "    \n",
    "    weight_amuse = (1/amuse_num) * (total/4.0)\n",
    "    weight_normal = (1/normal_num) * (total/4.0)\n",
    "    weight_stress = (1/stress_num) * (total/4.0)\n",
    "    weight_meditation  = (1/meditation_num) * (total/4.0)\n",
    "    class_weight = {0:weight_amuse, 1:weight_normal, 2:weight_stress, 3:weight_meditation}\n",
    "    # print(total, class_weight)\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "def Plot_lr_curve(history):\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy'] \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "def Plot_confusion_matrix(true_d, pred, class_name):\n",
    "    \n",
    "    pred_label = []\n",
    "    true_label = []\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_label.append(np.argmax(pred[i]))\n",
    "        \n",
    "    for i in range(len(true_d)):\n",
    "        true_label.append(np.argmax(true_d[i]))\n",
    "    \n",
    "    print(pred_label[0:10])\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_label, pred_label)\n",
    "    print('accuracy : ', np.round(accuracy,3))\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(true_label, pred_label, average='macro')\n",
    "    print('precision : ', np.round(precision,3))\n",
    "    \n",
    "    # Recall\n",
    "    recall = recall_score(true_label, pred_label,average='macro')\n",
    "    print('recall : ', np.round(recall,3))\n",
    "    \n",
    "    # F1 Score\n",
    "    F1_score = f1_score(true_label, pred_label,average='macro')\n",
    "    print('F1_score : ', np.round(F1_score,3))\n",
    "\n",
    "    #clasification report\n",
    "    report = classification_report(true_label, pred_label, target_names=class_name)#'Amuse',\n",
    "    print(report)\n",
    "    \n",
    "    #confusion matrix\n",
    "    confusion = confusion_matrix(true_label, pred_label)\n",
    "    disp = ConfusionMatrixDisplay(confusion, display_labels=class_name)#'Amuse',\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    \n",
    "    #plt.figure(figsize=(10,5))\n",
    "    cmn = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=class_name, yticklabels=class_name)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3012740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle5 as pickle\n",
    "\n",
    "def load_data(db_path, segment, window=0):\n",
    "\n",
    "    back_0_data, back_2_data, back_3_data, resting_data = [], [], [], []\n",
    "    \n",
    "    for i, p in enumerate(os.listdir(db_path)):\n",
    "        \n",
    "        # with open(db_path+p,\"rb\") as fr:\n",
    "        #     df = pickle.load(fr, encoding='bytes')\n",
    "            \n",
    "        df = pd.read_pickle(db_path + p)\n",
    "        # print(p)\n",
    "        back_0 = np.concatenate((df['process Trial 1:0back'], df['process Trial 6:0back']))\n",
    "        back_2 = np.concatenate((df['process Trial 2:2back'], df['process Trial 4:2back']))\n",
    "        back_3 = np.concatenate((df['process Trial 3:3back'], df['process Trial 5:3back']))\n",
    "        resting = df['Resting'].flatten()\n",
    "    \n",
    "        # print(back_0.shape, back_2.shape, back_3.shape, resting.shape)\n",
    "    \n",
    "        back_0_Rpeak = get_rpeak(back_0, sampling_rate)\n",
    "        back_2_Rpeak = get_rpeak(back_2, sampling_rate)\n",
    "        back_3_Rpeak = get_rpeak(back_3, sampling_rate)\n",
    "        resting_Rpeak = get_rpeak(resting.flatten().flatten(), sampling_rate)\n",
    "    \n",
    "        if segment == 'beat':\n",
    "            back_0_data.append(load_BEAT_data(back_0, back_0_Rpeak))\n",
    "            back_2_data.append(load_BEAT_data(back_2, back_2_Rpeak))\n",
    "            back_3_data.append(load_BEAT_data(back_3, back_3_Rpeak))\n",
    "            resting_data.append(load_BEAT_data(resting, resting_Rpeak))\n",
    "            \n",
    "        else:\n",
    "            back_0_data.append(load_RHYTHM_data(back_0, back_0_Rpeak, window))\n",
    "            back_2_data.append(load_RHYTHM_data(back_2, back_2_Rpeak, window))\n",
    "            back_3_data.append(load_RHYTHM_data(back_3, back_3_Rpeak, window))\n",
    "            resting_data.append(load_RHYTHM_data(resting, resting_Rpeak, window))\n",
    "\n",
    "    back_0_data = np.vstack(back_0_data)\n",
    "    back_2_data = np.vstack(back_2_data)\n",
    "    back_3_data = np.vstack(back_3_data)\n",
    "    resting_data = np.vstack(resting_data)\n",
    "\n",
    "    return back_0_data, back_2_data, back_3_data, resting_data\n",
    "\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaf491",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_sec = 0.24\n",
    "right_sec = 0.4\n",
    "\n",
    "sampling_rate=256\n",
    "\n",
    "class_name = ['0_back', '2_back', '3_back', 'Resting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "db_path = \"D:/Journal/MAUS_preprocessd_DB/\"\n",
    "person = os.listdir(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f16e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_list = [i*sampling_rate for i in range(2,11)]\n",
    "print(window_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b_back_0, b_back_2, b_back_3, b_resting = load_data(db_path, 'beat')\n",
    "r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', sampling_rate*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "resting_label = np.ones(r_resting.shape[0], np.int32) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e359e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beat_data = np.concatenate([b_back_0, b_back_2, b_back_3, b_resting])\n",
    "rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b6f40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beat, Beat_label = shuffle_data(rhythm_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9b91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b, test_b, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "train_b, val_b, train_label_y, val_label_y = train_test_split(train_b, train_label_y,\n",
    "                                                         test_size=0.25,\n",
    "                                                         stratify=train_label_y,\n",
    "                                                         random_state=128,\n",
    "                                                         shuffle=True)\n",
    "\n",
    "print(train_b.shape, val_b.shape, test_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7084fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = to_categorical(train_label_y)\n",
    "val_label = to_categorical(val_label_y)\n",
    "test_label = to_categorical(test_label_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41661c7f-aaf2-4318-90b4-1f293e5785a6",
   "metadata": {},
   "source": [
    "# Rhythm 5-Fold\n",
    "## 5 layer, 3 seoncd\n",
    "## CNN-2LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca09bb-71d1-4801-afd0-8fef8d0566b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', sampling_rate*3)\n",
    "\n",
    "back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "# beat_data = np.concatenate([b_back_0, b_back_2, b_back_3, b_resting])\n",
    "rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "\n",
    "Rhythm, Rhythm_label = shuffle_data(rhythm_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b6a0e2-80d4-441a-b8a2-d3e3d38c23c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 가중치 초기화 \n",
    "weight_init = HeNormal(seed= 128)\n",
    "\n",
    "save_model_path = \"D:/Journal/MAUS_Model_2/Rhythm_network/\"\n",
    "\n",
    "def scheuler(epoch, lr):\n",
    "    if epoch < 41:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback_lr = tf.keras.callbacks.LearningRateScheduler(scheuler)\n",
    "\n",
    "def cal_class_weight(label):\n",
    "    \n",
    "    counts = np.bincount(label)\n",
    "    \n",
    "    amuse_num = counts[0]\n",
    "    normal_num = counts[1]\n",
    "    stress_num = counts[2]\n",
    "    meditation_num = counts[3]\n",
    "    total = amuse_num + normal_num + stress_num + meditation_num\n",
    "    \n",
    "    weight_amuse = (1/amuse_num) * (total/4.0)\n",
    "    weight_normal = (1/normal_num) * (total/4.0)\n",
    "    weight_stress = (1/stress_num) * (total/4.0)\n",
    "    weight_meditation  = (1/meditation_num) * (total/4.0)\n",
    "    class_weight = {0:weight_amuse, 1:weight_normal, 2:weight_stress, 3:weight_meditation}\n",
    "    # print(total, class_weight)\n",
    "\n",
    "    return class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b4d1fa-3f58-40b0-953b-0d11d933ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhythm_model(re_X_train_r):\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "\n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "\n",
    "\n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "\n",
    "\n",
    "    conv1_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_2)\n",
    "    conv2_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_3)\n",
    "    bn1_R_3 = BatchNormalization()(conv2_R_3)\n",
    "    act_R_3 = Activation('relu')(bn1_R_3)\n",
    "    max1_R_3 = MaxPooling1D(2,2)(act_R_3)\n",
    "\n",
    "\n",
    "    conv1_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_3)\n",
    "    conv2_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_4)\n",
    "    bn1_R_4 = BatchNormalization()(conv2_R_4)\n",
    "    act_R_4 = Activation('relu')(bn1_R_4)\n",
    "    max1_R_4 = MaxPooling1D(2,2)(act_R_4)\n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_4)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6579ec-39da-46ad-8a1c-72c513486e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number = 1\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=128)\n",
    "\n",
    "for train_index, test_index in skf.split(Rhythm, Rhythm_label):\n",
    "    X_train_b, X_test_b = Rhythm[train_index], Rhythm[test_index]\n",
    "    Y_train, Y_test = Rhythm_label[train_index], Rhythm_label[test_index]\n",
    "\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_test = to_categorical(Y_test)\n",
    "    class_weight = cal_class_weight(Y_train)\n",
    "\n",
    "    MAUS_beat_model = rhythm_model(X_train_b.shape[1])\n",
    "    callback_lr = tf.keras.callbacks.LearningRateScheduler(scheuler)\n",
    "    MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_beat_model.fit(X_train_b, y_train, \n",
    "                        validation_data=(X_test_b, y_test), \n",
    "                        epochs=50, \n",
    "                        batch_size=32,\n",
    "                        callbacks=[callback_lr],\n",
    "                        class_weight=class_weight\n",
    "                       ) \n",
    "    \n",
    "    \n",
    "    scores = MAUS_beat_model.evaluate(X_test_b, y_test)\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_beat_model.predict(X_test_b)\n",
    "    Plot_confusion_matrix(y_test, pred, class_name)\n",
    "    \n",
    "    \n",
    "    MAUS_beat_model.save(save_model_path + 'rhythm_8020_5_layer_(0.0001)_Fold_'  +str(number)+ '_(' + str(np.round(scores[1]*100,2)) + ').h5')\n",
    "    number+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91deaa32-28e4-4d34-9fce-6aa04f609290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fe617-05a0-4d52-bb7d-18448ed2562d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41a77c51-43f7-4e28-ba60-68df04a982e0",
   "metadata": {},
   "source": [
    "# Fusion Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f873a87-575d-45ff-ac61-fe3132e2d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_back_0, b_back_2, b_back_3, b_resting = load_data(db_path, 'beat')\n",
    "r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', sampling_rate*3)\n",
    "\n",
    "b_back_0 = b_back_0[:r_back_0.shape[0]]\n",
    "b_back_2 = b_back_2[:r_back_2.shape[0]]\n",
    "b_back_3 = b_back_3[:r_back_3.shape[0]]\n",
    "b_resting = b_resting[:r_resting.shape[0]]\n",
    "\n",
    "back_0_label = np.zeros(b_back_0.shape[0], np.int32)\n",
    "back_2_label = np.ones(b_back_2.shape[0], np.int32) * 1\n",
    "back_3_label = np.ones(b_back_3.shape[0], np.int32) * 2\n",
    "resting_label = np.ones(b_resting.shape[0], np.int32) * 3\n",
    "\n",
    "beat_data = np.concatenate([b_back_0, b_back_2, b_back_3, b_resting])\n",
    "rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "\n",
    "Beat, Rhythm, Label = shuffle_data2(beat_data, rhythm_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c6eb2-0c85-4a28-9f97-4a1c6e154b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Beat.shape, Rhythm.shape, Label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba37221-2b72-40c3-9f12-e202df2748c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Average, maximum, Multiply, Add, dot\n",
    "\n",
    "def fusion_model_(re_X_train_b, re_X_train_r):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "\n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "\n",
    "\n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)\n",
    "\n",
    "    conv1_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_2)\n",
    "    conv2_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_3)\n",
    "    bn1_B_3 = BatchNormalization()(conv2_B_3)\n",
    "    act_B_3 = Activation('relu')(bn1_B_3)\n",
    "    max1_B_3 = MaxPooling1D(2,2)(act_B_3)\n",
    "\n",
    "    conv1_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_3)\n",
    "    conv2_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_4)\n",
    "    bn1_B_4 = BatchNormalization()(conv2_B_4)\n",
    "    act_B_4 = Activation('relu')(bn1_B_4)\n",
    "    max1_B_4 = MaxPooling1D(2,2)(act_B_4)\n",
    "\n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_4)\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "\n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "\n",
    "\n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "\n",
    "\n",
    "    conv1_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_2)\n",
    "    conv2_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_3)\n",
    "    bn1_R_3 = BatchNormalization()(conv2_R_3)\n",
    "    act_R_3 = Activation('relu')(bn1_R_3)\n",
    "    max1_R_3 = MaxPooling1D(2,2)(act_R_3)\n",
    "\n",
    "\n",
    "    conv1_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_3)\n",
    "    conv2_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_4)\n",
    "    bn1_R_4 = BatchNormalization()(conv2_R_4)\n",
    "    act_R_4 = Activation('relu')(bn1_R_4)\n",
    "    max1_R_4 = MaxPooling1D(2,2)(act_R_4)\n",
    "\n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_4)\n",
    "\n",
    "    #fusion\n",
    "    # concatenated = Concatenate()([GAP_B, GAP_R])\n",
    "    add_layer = maximum([GAP_B, GAP_R])\n",
    "\n",
    "    re_R = Reshape((1, add_layer.shape[1]))(add_layer)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=[input_beat, input_rhythm], outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f18b659-0766-45ea-810c-76e8d6e27491",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=128)\n",
    "\n",
    "save_model_path = \"D:/Journal/MAUS_Model_2/Fusion_network/\"\n",
    "number = 1\n",
    "\n",
    "for train_index, test_index in skf.split(Beat, Label):\n",
    "    print('=========================================__Fold__'+str(number)+'_______=================================')\n",
    "    X_train_b, X_test_b, X_train_r, X_test_r = Beat[train_index], Beat[test_index], Rhythm[train_index], Rhythm[test_index]\n",
    "    Y_train, Y_test = Label[train_index], Label[test_index]\n",
    "\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_test = to_categorical(Y_test)\n",
    "    class_weight = cal_class_weight(Y_train)\n",
    "    \n",
    "    MAUS_model = fusion_model_(X_train_b.shape[1], X_train_r.shape[1])\n",
    "    callback_lr = tf.keras.callbacks.LearningRateScheduler(scheuler)\n",
    "    MAUS_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_model.fit([X_train_b, X_train_r], y_train, \n",
    "                        validation_data=([X_test_b, X_test_r], y_test), \n",
    "                        epochs=50,\n",
    "                        batch_size=32,\n",
    "                        callbacks=[callback_lr],\n",
    "                        class_weight = class_weight\n",
    "                       ) \n",
    "\n",
    "    \n",
    "    scores = MAUS_model.evaluate([X_test_b,X_test_r], y_test)\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_model.predict([X_test_b,X_test_r])\n",
    "    Plot_confusion_matrix(y_test, pred, class_name)\n",
    "\n",
    "    \n",
    "    MAUS_model.save(save_model_path + 'MAUS_Fusion_Maximum_8020_Fold_'  +str(number)+ '_(' + str(np.round(scores[1]*100,2)) + ').h5')\n",
    "    number+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7afddd-2e35-4d5f-a3e5-63a2267ad2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2aaec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9320673c",
   "metadata": {},
   "source": [
    "# Beat layer setting experiment \n",
    "## layer (1-7), second (2-10) experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61d3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# 조기 종료 \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "\n",
    "# 가중치 초기화 \n",
    "weight_init = HeNormal(seed= 128)\n",
    "\n",
    "save_model_path = \"D:/JH/Journal/MAUS_Model/Rhythm_layer_setting/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b4e6d2",
   "metadata": {},
   "source": [
    "#  layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhythm_model_1_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dcb051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_1_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_layer(1)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8e051",
   "metadata": {},
   "source": [
    "# layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4922435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  layer 2\n",
    "\n",
    "def rhythm_model_2_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "    \n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_1)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d0611d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_2_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_'+str(window)+'_layer(2)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f6b86",
   "metadata": {},
   "source": [
    "# layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f865ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  layer 3\n",
    "\n",
    "def rhythm_model_3_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "    \n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "    \n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_2)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3adbbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_3_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_'+str(window)+'_layer(3)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1aa56",
   "metadata": {},
   "source": [
    "# layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ca17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  layer 4\n",
    "\n",
    "def rhythm_model_4_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "    \n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "    \n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "    \n",
    "    conv1_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_2)\n",
    "    conv2_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_3)\n",
    "    bn1_R_3 = BatchNormalization()(conv2_R_3)\n",
    "    act_R_3 = Activation('relu')(bn1_R_3)\n",
    "    max1_R_3 = MaxPooling1D(2,2)(act_R_3)\n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_3)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a242443",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_4_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_'+str(window)+'_layer(4)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73083a95",
   "metadata": {},
   "source": [
    "# layer 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  layer 5\n",
    "\n",
    "def rhythm_model_5_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "    \n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "    \n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "    \n",
    "    conv1_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_2)\n",
    "    conv2_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_3)\n",
    "    bn1_R_3 = BatchNormalization()(conv2_R_3)\n",
    "    act_R_3 = Activation('relu')(bn1_R_3)\n",
    "    max1_R_3 = MaxPooling1D(2,2)(act_R_3)\n",
    "    \n",
    "    conv1_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_3)\n",
    "    conv2_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_4)\n",
    "    bn1_R_4 = BatchNormalization()(conv2_R_4)\n",
    "    act_R_4 = Activation('relu')(bn1_R_4)\n",
    "    max1_R_4 = MaxPooling1D(2,2)(act_R_4)    \n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_4)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c26649",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_5_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_'+str(window)+'_layer(5)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73e6445",
   "metadata": {},
   "source": [
    "# layer 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867fed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  layer 6\n",
    "\n",
    "def rhythm_model_6_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "    \n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "    \n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "    \n",
    "    conv1_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_2)\n",
    "    conv2_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_3)\n",
    "    bn1_R_3 = BatchNormalization()(conv2_R_3)\n",
    "    act_R_3 = Activation('relu')(bn1_R_3)\n",
    "    max1_R_3 = MaxPooling1D(2,2)(act_R_3)\n",
    "    \n",
    "    conv1_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_3)\n",
    "    conv2_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_4)\n",
    "    bn1_R_4 = BatchNormalization()(conv2_R_4)\n",
    "    act_R_4 = Activation('relu')(bn1_R_4)\n",
    "    max1_R_4 = MaxPooling1D(2,2)(act_R_4)    \n",
    "\n",
    "    conv1_R_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_4)\n",
    "    conv2_R_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_5)\n",
    "    bn1_R_5 = BatchNormalization()(conv2_R_5)\n",
    "    act_R_5 = Activation('relu')(bn1_R_5)\n",
    "    max1_R_5 = MaxPooling1D(2,2)(act_R_5)     \n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_5)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265d1384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_6_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_'+str(window)+'_layer(6)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294cb64",
   "metadata": {},
   "source": [
    "# layer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0df3f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  layer 7\n",
    "\n",
    "def rhythm_model_7_layer(re_X_train_r):\n",
    "\n",
    "    input_rhythm = Input(shape=(re_X_train_r,1), name = 'input_rhythm')\n",
    "\n",
    "    # Rhythm\n",
    "\n",
    "    conv1_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_rhythm)\n",
    "    conv2_R = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R)\n",
    "    bn1_R = BatchNormalization()(conv2_R)\n",
    "    act_R = Activation('relu')(bn1_R)\n",
    "    max1_R = MaxPooling1D(2,2)(act_R)\n",
    "    \n",
    "    conv1_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R)\n",
    "    conv2_R_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_1)\n",
    "    bn1_R_1 = BatchNormalization()(conv2_R_1)\n",
    "    act_R_1 = Activation('relu')(bn1_R_1)\n",
    "    max1_R_1 = MaxPooling1D(2,2)(act_R_1)\n",
    "    \n",
    "    conv1_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_1)\n",
    "    conv2_R_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_2)\n",
    "    bn1_R_2 = BatchNormalization()(conv2_R_2)\n",
    "    act_R_2 = Activation('relu')(bn1_R_2)\n",
    "    max1_R_2 = MaxPooling1D(2,2)(act_R_2)\n",
    "    \n",
    "    conv1_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_2)\n",
    "    conv2_R_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_3)\n",
    "    bn1_R_3 = BatchNormalization()(conv2_R_3)\n",
    "    act_R_3 = Activation('relu')(bn1_R_3)\n",
    "    max1_R_3 = MaxPooling1D(2,2)(act_R_3)\n",
    "    \n",
    "    conv1_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_3)\n",
    "    conv2_R_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_4)\n",
    "    bn1_R_4 = BatchNormalization()(conv2_R_4)\n",
    "    act_R_4 = Activation('relu')(bn1_R_4)\n",
    "    max1_R_4 = MaxPooling1D(2,2)(act_R_4)    \n",
    "\n",
    "    conv1_R_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_4)\n",
    "    conv2_R_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_5)\n",
    "    bn1_R_5 = BatchNormalization()(conv2_R_5)\n",
    "    act_R_5 = Activation('relu')(bn1_R_5)\n",
    "    max1_R_5 = MaxPooling1D(2,2)(act_R_5)     \n",
    "    \n",
    "    conv1_R_6 = Conv1D(512, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_R_5)\n",
    "    conv2_R_6 = Conv1D(512, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_R_6)\n",
    "    bn1_R_6 = BatchNormalization()(conv2_R_6)\n",
    "    act_R_6 = Activation('relu')(bn1_R_6)\n",
    "    max1_R_6 = MaxPooling1D(2,2)(act_R_6)      \n",
    "\n",
    "    GAP_R = GlobalAveragePooling1D()(max1_R_6)\n",
    "\n",
    "    re_R = Reshape((1, GAP_R.shape[1]))(GAP_R)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs=input_rhythm, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25014e2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for window in window_list:\n",
    "    print()\n",
    "    print(f\"===================={str(window)}=====================\")\n",
    "    print()\n",
    "    r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm', window)\n",
    "\n",
    "    back_0_label = np.zeros(r_back_0.shape[0], np.int32)\n",
    "    back_2_label = np.ones(r_back_2.shape[0], np.int32) * 1\n",
    "    back_3_label = np.ones(r_back_3.shape[0], np.int32) * 2\n",
    "    resting_label = np.ones(r_resting.shape[0], np.int32) * 3\n",
    "\n",
    "    rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "    label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])\n",
    "    \n",
    "    Beat, Beat_label = shuffle_data(rhythm_data, label)\n",
    "    \n",
    "    train_r, test_r, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "    train_r, val_r, train_label_y, val_label_y = train_test_split(train_r, train_label_y,\n",
    "                                                             test_size=0.25,\n",
    "                                                             stratify=train_label_y,\n",
    "                                                             random_state=128,\n",
    "                                                             shuffle=True)\n",
    "    \n",
    "    train_label = to_categorical(train_label_y)\n",
    "    val_label = to_categorical(val_label_y)\n",
    "    test_label = to_categorical(test_label_y)\n",
    "    class_weight = cal_class_weight(train_label_y)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "    \n",
    "    # model\n",
    "    MAUS_rhythm_model = rhythm_model_7_layer(train_r.shape[1])\n",
    "    MAUS_rhythm_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_rhythm_model.fit(train_r, train_label, \n",
    "                              validation_data=(val_r, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             )\n",
    "    \n",
    "    scores = MAUS_rhythm_model.evaluate(test_r, test_label)\n",
    "    MAUS_rhythm_model.save(save_model_path + 'MAUS_Rhythm_(622)_'+str(window)+'_layer(7)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_rhythm_model.predict(test_r)\n",
    "    Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec0c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd74a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_research",
   "language": "python",
   "name": "ecg1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
