{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import neurokit2 as nk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Conv1D,TimeDistributed, Reshape, Activation, add, Layer, Concatenate, GRU, Bidirectional \n",
    "from tensorflow.keras.layers import MaxPooling1D, BatchNormalization, Dropout,Activation, GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba8c395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-peak detection\n",
    "def get_rpeak(signal, sampling_rate):\n",
    "    _, rpeaks = nk.ecg_peaks(signal, sampling_rate=sampling_rate, show=False, method='neurokit')\n",
    "    \n",
    "    return rpeaks['ECG_R_Peaks']\n",
    "\n",
    "# Segmentation Function\n",
    "\n",
    "def load_BEAT_data(signal, r_peaks):\n",
    "    signal_list = []\n",
    "    for number in range(1,len(r_peaks)-1):\n",
    "        signal_list.append(signal[r_peaks[number]-(int(left_sec*sampling_rate)) : r_peaks[number]+(int(right_sec*sampling_rate))])\n",
    "    return np.array(signal_list)\n",
    "\n",
    "def load_RHYTHM_data(data, rpeak, window):\n",
    "    rhythm_data = []\n",
    "    right = int(right_sec * sampling_rate)\n",
    "    for i in range(1,len(rpeak)-1):\n",
    "        if len(data[rpeak[i] + right : rpeak[i] + window + right]) == window:\n",
    "            rhythm_data.append(data[rpeak[i] + right : rpeak[i]+right + window])\n",
    "\n",
    "    return np.array(rhythm_data)\n",
    "\n",
    "def shuffle_data(data, labels):\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return data[indices], labels[indices]\n",
    "\n",
    "def shuffle_data2(data1, data2, labels):\n",
    "    # data1, data2, label을 입력받아 무작위로 섞고 (인덱스 위치는 동일하게) Return\n",
    "    indices = np.arange(data1.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    return data1[indices], data2[indices], labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845da62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_class_weight(label):\n",
    "    \n",
    "    counts = np.bincount(label)\n",
    "    \n",
    "    amuse_num = counts[0]\n",
    "    normal_num = counts[1]\n",
    "    stress_num = counts[2]\n",
    "    meditation_num = counts[3]\n",
    "    total = amuse_num + normal_num + stress_num + meditation_num\n",
    "    \n",
    "    weight_amuse = (1/amuse_num) * (total/4.0)\n",
    "    weight_normal = (1/normal_num) * (total/4.0)\n",
    "    weight_stress = (1/stress_num) * (total/4.0)\n",
    "    weight_meditation  = (1/meditation_num) * (total/4.0)\n",
    "    class_weight = {0:weight_amuse, 1:weight_normal, 2:weight_stress, 3:weight_meditation}\n",
    "    # print(total, class_weight)\n",
    "\n",
    "    return class_weight\n",
    "\n",
    "def Plot_lr_curve(history):\n",
    "    \n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy'] \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(121)\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "def Plot_confusion_matrix(true_d, pred, class_name):\n",
    "    \n",
    "    pred_label = []\n",
    "    true_label = []\n",
    "    \n",
    "    for i in range(len(pred)):\n",
    "        pred_label.append(np.argmax(pred[i]))\n",
    "        \n",
    "    for i in range(len(true_d)):\n",
    "        true_label.append(np.argmax(true_d[i]))\n",
    "    \n",
    "    print(pred_label[0:10])\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(true_label, pred_label)\n",
    "    print('accuracy : ', np.round(accuracy,3))\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(true_label, pred_label, average='macro')\n",
    "    print('precision : ', np.round(precision,3))\n",
    "    \n",
    "    # Recall\n",
    "    recall = recall_score(true_label, pred_label,average='macro')\n",
    "    print('recall : ', np.round(recall,3))\n",
    "    \n",
    "    # F1 Score\n",
    "    F1_score = f1_score(true_label, pred_label,average='macro')\n",
    "    print('F1_score : ', np.round(F1_score,3))\n",
    "\n",
    "    #clasification report\n",
    "    report = classification_report(true_label, pred_label, target_names=class_name)#'Amuse',\n",
    "    print(report)\n",
    "    \n",
    "    #confusion matrix\n",
    "    confusion = confusion_matrix(true_label, pred_label)\n",
    "    disp = ConfusionMatrixDisplay(confusion, display_labels=class_name)#'Amuse',\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.show()\n",
    "    \n",
    "    #plt.figure(figsize=(10,5))\n",
    "    cmn = confusion.astype('float') / confusion.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cmn, annot=True, fmt='.2f', xticklabels=class_name, yticklabels=class_name)\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "\n",
    "def load_data(db_path, segment, window=0):\n",
    "\n",
    "    back_0_data, back_2_data, back_3_data, resting_data = [], [], [], []\n",
    "    \n",
    "    for i, p in enumerate(os.listdir(db_path)):\n",
    "        \n",
    "        with open(db_path+p,\"rb\") as fr:\n",
    "            df = pickle.load(fr, encoding='bytes')\n",
    "            \n",
    "#         df = pd.read_pickle(db_path + p)\n",
    "        # print(p)\n",
    "        back_0 = np.concatenate((df['process Trial 1:0back'], df['process Trial 6:0back']))\n",
    "        back_2 = np.concatenate((df['process Trial 2:2back'], df['process Trial 4:2back']))\n",
    "        back_3 = np.concatenate((df['process Trial 3:3back'], df['process Trial 5:3back']))\n",
    "        resting = df['Resting'].flatten()\n",
    "    \n",
    "        # print(back_0.shape, back_2.shape, back_3.shape, resting.shape)\n",
    "    \n",
    "        back_0_Rpeak = get_rpeak(back_0, sampling_rate)\n",
    "        back_2_Rpeak = get_rpeak(back_2, sampling_rate)\n",
    "        back_3_Rpeak = get_rpeak(back_3, sampling_rate)\n",
    "        resting_Rpeak = get_rpeak(resting.flatten().flatten(), sampling_rate)\n",
    "    \n",
    "        if segment == 'beat':\n",
    "            back_0_data.append(load_BEAT_data(back_0, back_0_Rpeak))\n",
    "            back_2_data.append(load_BEAT_data(back_2, back_2_Rpeak))\n",
    "            back_3_data.append(load_BEAT_data(back_3, back_3_Rpeak))\n",
    "            resting_data.append(load_BEAT_data(resting, resting_Rpeak))\n",
    "            \n",
    "        else:\n",
    "            back_0_data.append(load_RHYTHM_data(back_0, back_0_Rpeak, window))\n",
    "            back_2_data.append(load_RHYTHM_data(back_2, back_2_Rpeak, window))\n",
    "            back_3_data.append(load_RHYTHM_data(back_3, back_3_Rpeak, window))\n",
    "            resting_data.append(load_RHYTHM_data(resting, resting_Rpeak, window))\n",
    "\n",
    "    back_0_data = np.vstack(back_0_data)\n",
    "    back_2_data = np.vstack(back_2_data)\n",
    "    back_3_data = np.vstack(back_3_data)\n",
    "    resting_data = np.vstack(resting_data)\n",
    "\n",
    "    return back_0_data, back_2_data, back_3_data, resting_data\n",
    "\n",
    "\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda60dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_sec = 0.24\n",
    "right_sec = 0.4\n",
    "\n",
    "sampling_rate=256\n",
    "\n",
    "class_name = ['0_back', '2_back', '3_back', 'Resting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ad9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = \"D:/JH/Journal/MAUS_preprocessd/\"\n",
    "person = os.listdir(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2a72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_back_0, b_back_2, b_back_3, b_resting = load_data(db_path, 'beat')\n",
    "# r_back_0, r_back_2, r_back_3, r_resting = load_data(db_path, 'rhythm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_0_label = np.zeros(b_back_0.shape[0], np.int32)\n",
    "back_2_label = np.ones(b_back_2.shape[0], np.int32) * 1\n",
    "back_3_label = np.ones(b_back_3.shape[0], np.int32) * 2\n",
    "resting_label = np.ones(b_resting.shape[0], np.int32) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd84a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "beat_data = np.concatenate([b_back_0, b_back_2, b_back_3, b_resting])\n",
    "# rhythm_data = np.concatenate([r_back_0, r_back_2, r_back_3, r_resting])\n",
    "label = np.concatenate([back_0_label, back_2_label, back_3_label, resting_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "Beat, Beat_label = shuffle_data(beat_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9cc5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_b, test_b, train_label_y, test_label_y = train_test_split(Beat, Beat_label,\n",
    "                                                            test_size=0.2,\n",
    "                                                            stratify=Beat_label,\n",
    "                                                            random_state=128,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "train_b, val_b, train_label_y, val_label_y = train_test_split(train_b, train_label_y,\n",
    "                                                         test_size=0.25,\n",
    "                                                         stratify=train_label_y,\n",
    "                                                         random_state=128,\n",
    "                                                         shuffle=True)\n",
    "\n",
    "print(train_b.shape, val_b.shape, test_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e85c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = to_categorical(train_label_y)\n",
    "val_label = to_categorical(val_label_y)\n",
    "test_label = to_categorical(test_label_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbeb15f",
   "metadata": {},
   "source": [
    "# 1 layer 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d01ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# 조기 종료 \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=0.005, restore_best_weights=True)\n",
    "\n",
    "# 가중치 초기화 \n",
    "weight_init = HeNormal(seed= 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path = \"D:/JH/Journal/MAUS_Model/Beat_layer_setting/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c7e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_1_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdcc62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_1_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(1)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e486b1",
   "metadata": {},
   "source": [
    "# 2 layer 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7bb856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_2_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_1)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5d324a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_2_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(2)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c7aa98",
   "metadata": {},
   "source": [
    "# 3 layer 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16204cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_3_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)    \n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_2)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf2164",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_3_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(3)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5853b0",
   "metadata": {},
   "source": [
    "# 4 layer 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60509fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_4_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)    \n",
    "    \n",
    "    conv1_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_2)\n",
    "    conv2_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_3)\n",
    "    bn1_B_3 = BatchNormalization()(conv2_B_3)\n",
    "    act_B_3 = Activation('relu')(bn1_B_3)\n",
    "    max1_B_3 = MaxPooling1D(2,2)(act_B_3)    \n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_3)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecee43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_4_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(4)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfc05a6",
   "metadata": {},
   "source": [
    "# 5 layer 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf23d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_5_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)    \n",
    "    \n",
    "    conv1_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_2)\n",
    "    conv2_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_3)\n",
    "    bn1_B_3 = BatchNormalization()(conv2_B_3)\n",
    "    act_B_3 = Activation('relu')(bn1_B_3)\n",
    "    max1_B_3 = MaxPooling1D(2,2)(act_B_3)   \n",
    "    \n",
    "    conv1_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_3)\n",
    "    conv2_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_4)\n",
    "    bn1_B_4 = BatchNormalization()(conv2_B_4)\n",
    "    act_B_4 = Activation('relu')(bn1_B_4)\n",
    "    max1_B_4 = MaxPooling1D(2,2)(act_B_4) \n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_4)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdebc51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_5_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(5)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc3392",
   "metadata": {},
   "source": [
    "# 6 layer 실험"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43773a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_6_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)    \n",
    "    \n",
    "    conv1_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_2)\n",
    "    conv2_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_3)\n",
    "    bn1_B_3 = BatchNormalization()(conv2_B_3)\n",
    "    act_B_3 = Activation('relu')(bn1_B_3)\n",
    "    max1_B_3 = MaxPooling1D(2,2)(act_B_3)   \n",
    "    \n",
    "    conv1_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_3)\n",
    "    conv2_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_4)\n",
    "    bn1_B_4 = BatchNormalization()(conv2_B_4)\n",
    "    act_B_4 = Activation('relu')(bn1_B_4)\n",
    "    max1_B_4 = MaxPooling1D(2,2)(act_B_4) \n",
    "    \n",
    "    conv1_B_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_4)\n",
    "    conv2_B_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_5)\n",
    "    bn1_B_5 = BatchNormalization()(conv2_B_5)\n",
    "    act_B_5 = Activation('relu')(bn1_B_5)\n",
    "    max1_B_5 = MaxPooling1D(2,2)(act_B_5) \n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_5)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813da842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_6_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(6)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462eda0",
   "metadata": {},
   "source": [
    "# layer 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374b4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_7_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)    \n",
    "    \n",
    "    conv1_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_2)\n",
    "    conv2_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_3)\n",
    "    bn1_B_3 = BatchNormalization()(conv2_B_3)\n",
    "    act_B_3 = Activation('relu')(bn1_B_3)\n",
    "    max1_B_3 = MaxPooling1D(2,2)(act_B_3)   \n",
    "    \n",
    "    conv1_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_3)\n",
    "    conv2_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_4)\n",
    "    bn1_B_4 = BatchNormalization()(conv2_B_4)\n",
    "    act_B_4 = Activation('relu')(bn1_B_4)\n",
    "    max1_B_4 = MaxPooling1D(2,2)(act_B_4) \n",
    "    \n",
    "    conv1_B_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_4)\n",
    "    conv2_B_5 = Conv1D(256, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_5)\n",
    "    bn1_B_5 = BatchNormalization()(conv2_B_5)\n",
    "    act_B_5 = Activation('relu')(bn1_B_5)\n",
    "    max1_B_5 = MaxPooling1D(2,2)(act_B_5) \n",
    "    \n",
    "    conv1_B_6 = Conv1D(512, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_5)\n",
    "    conv2_B_6 = Conv1D(512, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_6)\n",
    "    bn1_B_6 = BatchNormalization()(conv2_B_6)\n",
    "    act_B_6 = Activation('relu')(bn1_B_6)\n",
    "    max1_B_6 = MaxPooling1D(2,2)(act_B_6)     \n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_6)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a0ad7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_weight = cal_class_weight(train_label_y)\n",
    "print(class_weight)\n",
    "\n",
    "MAUS_beat_model = beat_model_7_layer(train_b.shape[1])\n",
    "MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = MAUS_beat_model.fit(train_b, train_label, \n",
    "                              validation_data=(val_b, val_label), \n",
    "                              epochs=200, \n",
    "                              batch_size=32,\n",
    "                              callbacks=[early_stopping],\n",
    "                              class_weight=class_weight\n",
    "                             ) \n",
    "scores = MAUS_beat_model.evaluate(test_b, test_label)\n",
    "MAUS_beat_model.save(save_model_path + 'MAUS_beat_(622)_layer(7)_(' + str(np.round(scores[1]*100,2)) +').h5' )\n",
    "Plot_lr_curve(history)\n",
    "pred = MAUS_beat_model.predict(test_b)\n",
    "Plot_confusion_matrix(test_label, pred, class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b529fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ace53736",
   "metadata": {},
   "source": [
    "# Beat 5-Fold 실험"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e765e00",
   "metadata": {},
   "source": [
    "### Train 80, Test 20, 학습률 0.00001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3e10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beat_model_5_layer(re_X_train_b):\n",
    "    input_beat = Input(shape=(re_X_train_b,1), name = 'input_beat')\n",
    "\n",
    "    conv1_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(input_beat)\n",
    "    conv2_B = Conv1D(8, 5, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B)\n",
    "    bn1_B = BatchNormalization()(conv2_B)\n",
    "    act_B = Activation('relu')(bn1_B)\n",
    "    max1_B = MaxPooling1D(2,2)(act_B)\n",
    "    \n",
    "    conv1_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B)\n",
    "    conv2_B_1 = Conv1D(16, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_1)\n",
    "    bn1_B_1 = BatchNormalization()(conv2_B_1)\n",
    "    act_B_1 = Activation('relu')(bn1_B_1)\n",
    "    max1_B_1 = MaxPooling1D(2,2)(act_B_1)\n",
    "    \n",
    "    conv1_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_1)\n",
    "    conv2_B_2 = Conv1D(32, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_2)\n",
    "    bn1_B_2 = BatchNormalization()(conv2_B_2)\n",
    "    act_B_2 = Activation('relu')(bn1_B_2)\n",
    "    max1_B_2 = MaxPooling1D(2,2)(act_B_2)    \n",
    "    \n",
    "    conv1_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_2)\n",
    "    conv2_B_3 = Conv1D(64, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_3)\n",
    "    bn1_B_3 = BatchNormalization()(conv2_B_3)\n",
    "    act_B_3 = Activation('relu')(bn1_B_3)\n",
    "    max1_B_3 = MaxPooling1D(2,2)(act_B_3)   \n",
    "    \n",
    "    conv1_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(max1_B_3)\n",
    "    conv2_B_4 = Conv1D(128, 3, activation=None, padding='same', kernel_initializer=weight_init)(conv1_B_4)\n",
    "    bn1_B_4 = BatchNormalization()(conv2_B_4)\n",
    "    act_B_4 = Activation('relu')(bn1_B_4)\n",
    "    max1_B_4 = MaxPooling1D(2,2)(act_B_4) \n",
    "    \n",
    "    GAP_B = GlobalAveragePooling1D()(max1_B_4)\n",
    "\n",
    "    re_R = Reshape((1, GAP_B.shape[1]))(GAP_B)\n",
    "    x_R_LSTM = LSTM(128,return_sequences=True)(re_R)\n",
    "    x_R_LSTM2 = LSTM(64, return_sequences=True)(x_R_LSTM)\n",
    "    fl_f = Flatten()(x_R_LSTM2)\n",
    "\n",
    "    den1 = Dense(128, activation='relu')(fl_f)\n",
    "    den2 = Dense(64, activation='relu')(den1)\n",
    "\n",
    "    fusion_output = Dense(4, activation='softmax')(den2)\n",
    "\n",
    "    fusion_model = Model(inputs= input_beat, outputs= fusion_output)\n",
    "    fusion_model.summary()\n",
    "    \n",
    "    return fusion_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a340057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Beat.shape, Beat_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec96d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheuler(epoch, lr):\n",
    "    if epoch < 41:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "    \n",
    "callback_lr = tf.keras.callbacks.LearningRateScheduler(scheuler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db53b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=128)\n",
    "\n",
    "save_model_path = \"D:/JH/Journal/MAUS_Model/Beat_5Fold/\"\n",
    "number = 1\n",
    "\n",
    "for train_index, test_index in skf.split(Beat, Beat_label):\n",
    "    print('=========================================__Fold__'+str(number)+'_______=================================')\n",
    "    X_train_b, X_test_b = Beat[train_index], Beat[test_index]\n",
    "    Y_train, Y_test = Beat_label[train_index], Beat_label[test_index]\n",
    "\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_test = to_categorical(Y_test)\n",
    "    class_weight = cal_class_weight(Y_train)\n",
    "    \n",
    "    MAUS_beat_model = beat_model_5_layer(X_train_b.shape[1])\n",
    "    callback_lr = tf.keras.callbacks.LearningRateScheduler(scheuler)\n",
    "    MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_beat_model.fit(X_train_b, y_train, \n",
    "                        validation_data=(X_test_b, y_test), \n",
    "                        epochs=50, \n",
    "                        batch_size=32,\n",
    "                        callbacks=[callback_lr],\n",
    "                        class_weight=class_weight\n",
    "                       ) \n",
    "\n",
    "    \n",
    "    scores = MAUS_beat_model.evaluate(X_test_b, y_test)\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_beat_model.predict(X_test_b)\n",
    "    Plot_confusion_matrix(y_test, pred, class_name)\n",
    "\n",
    "    \n",
    "    MAUS_beat_model.save(save_model_path + 'MAUS_beat_8020_Fold('  +str(number)+ ')_(' + str(np.round(scores[1]*100,2)) + ').h5')\n",
    "    number+=1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628c372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1251f65",
   "metadata": {},
   "source": [
    "# 학습률 0.0001 적용 \n",
    "#### 0.00001에서 성능이 원활하게 나오지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a6d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=128)\n",
    "\n",
    "save_model_path = \"D:/JH/Journal/MAUS_Model/Beat_5Fold/\"\n",
    "number = 1\n",
    "\n",
    "for train_index, test_index in skf.split(Beat, Beat_label):\n",
    "    print('=========================================__Fold__'+str(number)+'_______=================================')\n",
    "    X_train_b, X_test_b = Beat[train_index], Beat[test_index]\n",
    "    Y_train, Y_test = Beat_label[train_index], Beat_label[test_index]\n",
    "\n",
    "    y_train = to_categorical(Y_train)\n",
    "    y_test = to_categorical(Y_test)\n",
    "    class_weight = cal_class_weight(Y_train)\n",
    "    \n",
    "    MAUS_beat_model = beat_model_5_layer(X_train_b.shape[1])\n",
    "    callback_lr = tf.keras.callbacks.LearningRateScheduler(scheuler)\n",
    "    MAUS_beat_model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = MAUS_beat_model.fit(X_train_b, y_train, \n",
    "                        validation_data=(X_test_b, y_test), \n",
    "                        epochs=50, \n",
    "                        batch_size=32,\n",
    "                        callbacks=[callback_lr],\n",
    "                        class_weight=class_weight\n",
    "                       ) \n",
    "\n",
    "    \n",
    "    scores = MAUS_beat_model.evaluate(X_test_b, y_test)\n",
    "    Plot_lr_curve(history)\n",
    "    pred = MAUS_beat_model.predict(X_test_b)\n",
    "    Plot_confusion_matrix(y_test, pred, class_name)\n",
    "\n",
    "    \n",
    "    MAUS_beat_model.save(save_model_path + 'MAUS_beat_8020_Fold('  +str(number)+ ')_(' + str(np.round(scores[1]*100,2)) + ')_0.0001_.h5')\n",
    "    number+=1\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg_research",
   "language": "python",
   "name": "ecg1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
